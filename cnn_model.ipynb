{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13384481,"sourceType":"datasetVersion","datasetId":8492537}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\n# Copy dataset from Kaggle input to working folder\nsrc = '/kaggle/input/fer2013-dataset'\ndst = '/kaggle/working/fer2013-dataset-copy'\n\nif not os.path.exists(dst):\n    shutil.copytree(src, dst)\n\n# Keep only emotions of interest\nkeep_classes = ['angry', 'happy', 'sad', 'surprise', 'neutral']\n\ndef clean_folders(base_dir, keep):\n    for folder in os.listdir(base_dir):\n        folder_path = os.path.join(base_dir, folder)\n        if os.path.isdir(folder_path) and folder not in keep:\n            shutil.rmtree(folder_path)\n\nclean_folders(os.path.join(dst, 'train'), keep_classes)\nclean_folders(os.path.join(dst, 'test'), keep_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:58:45.404138Z","iopub.execute_input":"2025-10-15T08:58:45.404413Z","iopub.status.idle":"2025-10-15T09:05:30.687138Z","shell.execute_reply.started":"2025-10-15T08:58:45.404356Z","shell.execute_reply":"2025-10-15T09:05:30.686584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\n\nbase_dir = '/kaggle/working/fer2013-dataset-copy'\ntrain_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'test')\n\nemotions = ['angry', 'happy', 'sad', 'surprise', 'neutral']\nimg_size = (224, 224)  # Match MobileNetV2 pretrained weights input size\nbatch_size = 32\n\ntrain_gen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2,\n    rotation_range=30,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_data = train_gen.flow_from_directory(\n    train_dir, target_size=img_size, batch_size=batch_size,\n    class_mode='categorical', subset='training', shuffle=True, classes=emotions\n)\n\nval_data = train_gen.flow_from_directory(\n    train_dir, target_size=img_size, batch_size=batch_size,\n    class_mode='categorical', subset='validation', shuffle=False, classes=emotions\n)\n\ntest_gen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntest_data = test_gen.flow_from_directory(\n    test_dir, target_size=img_size, batch_size=batch_size,\n    class_mode='categorical', shuffle=False, classes=emotions\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:36.227937Z","iopub.execute_input":"2025-10-15T09:05:36.228449Z","iopub.status.idle":"2025-10-15T09:05:56.725982Z","shell.execute_reply.started":"2025-10-15T09:05:36.228425Z","shell.execute_reply":"2025-10-15T09:05:56.725441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Multiply, Reshape, Activation\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\n\ndef se_block(input_tensor, ratio=16):\n    channel_axis = -1\n    filters = input_tensor.shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    se = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters // ratio, activation='selu', kernel_initializer='he_normal', use_bias=False)(se)\n    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n    x = Multiply()([input_tensor, se])\n    return x\n\nimg_size = (224, 224)\nemotions = ['angry', 'happy', 'sad', 'surprise', 'neutral']\n\ninputs = Input(shape=(*img_size, 3))\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nx = base_model.output\nx = se_block(x)\nx = Conv2D(64, (3, 3), padding='same')(x)\nx = Activation('selu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(128, (3, 3), padding='same')(x)\nx = Activation('selu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='selu')(x)\noutputs = Dense(len(emotions), activation='softmax')(x)\n\nmodel = Model(inputs, outputs)\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-4),\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:59.727499Z","iopub.execute_input":"2025-10-15T09:05:59.728198Z","iopub.status.idle":"2025-10-15T09:06:04.447743Z","shell.execute_reply.started":"2025-10-15T09:05:59.728175Z","shell.execute_reply":"2025-10-15T09:06:04.447044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=1e-6)\n\n# Train with frozen base model\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=30,\n    callbacks=[early_stopping, reduce_lr]\n)\n\n# Unfreeze last 60 layers (for stronger fine-tuning)\nfor layer in model.layers[-60:]:\n    layer.trainable = True\n\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-5),\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)\n\nhistory_finetune = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=40,\n    callbacks=[early_stopping, reduce_lr]\n)\n\n# Save the model after training\nmodel.save('fer2013_mobilenetv2_se_selu_finetuned.keras')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:06:16.503923Z","iopub.execute_input":"2025-10-15T09:06:16.504443Z","iopub.status.idle":"2025-10-15T13:51:44.403034Z","shell.execute_reply.started":"2025-10-15T09:06:16.504419Z","shell.execute_reply":"2025-10-15T13:51:44.402215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load model if starting fresh\nfrom tensorflow.keras.models import load_model\nmodel = load_model('fer2013_mobilenetv2_se_selu_finetuned.keras')\n\n# Evaluate on validation and test sets\nval_loss, val_acc = model.evaluate(val_data)\ntest_loss, test_acc = model.evaluate(test_data)\nprint(f\"Validation Accuracy: {val_acc*100:.2f}%\")\nprint(f\"Test Accuracy: {test_acc*100:.2f}%\")\n\n# Predictions for detailed metrics\ntest_data.reset()\ny_true = test_data.classes\ny_pred_probs = model.predict(test_data)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=test_data.class_indices.keys()))\n\n# Confusion matrix plot\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=test_data.class_indices.keys(),\n            yticklabels=test_data.class_indices.keys())\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for FER2013 Test Set')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T13:54:01.003176Z","iopub.execute_input":"2025-10-15T13:54:01.003880Z","iopub.status.idle":"2025-10-15T13:55:21.286646Z","shell.execute_reply.started":"2025-10-15T13:54:01.003854Z","shell.execute_reply":"2025-10-15T13:55:21.286012Z"}},"outputs":[],"execution_count":null}]}